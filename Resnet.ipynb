{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b17a4fc",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1dfb91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import os, sys\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "#!pip install six numpy scipy Pillow matplotlib scikit-image opencv-python imageio\n",
    "#!pip install --no-dependencies imgaug\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "WORKERS = 2\n",
    "CHANNEL = 3\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "IMG_SIZE = 512\n",
    "NUM_CLASSES = 18\n",
    "SEED = 77\n",
    "TRAIN_NUM = 1000 # use 1000 when you just want to explore new idea, use -1 for full train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5695d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/opt/ml/input/data/train/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1567e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_class = pd.DataFrame(columns = ['id', 'per_id', 'gender', 'age', 'mask', 'class', 'path'])\n",
    "df_train_class.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f5c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_class_simple(row, mask):\n",
    "    # Assuming the mask is already labeled as 0,1,2\n",
    "    # Each of them is 'wear', 'incorrect' and 'not wear'\n",
    "    gender = 0 if row[\"gender\"] == \"male\" else 3\n",
    "    age = min(2, row[\"age\"] // 30)\n",
    "\n",
    "    # Print the class number\n",
    "    return mask*6 + gender + age, age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd420e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../input/data/train/images'\n",
    "\n",
    "!rm -rf ./data/train/.DS_Store\n",
    "!rm -rf ./data/train/images/.DS_Store\n",
    "folders = sorted([f for f in os.listdir(path) if \"._\" not in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f7a278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "mask_dict = {0: 'wear', 1: 'not wear', 2: 'incorrect'}\n",
    "age_dict = {0: 'under 30', 1: '30 to 60', 2: 'over 60'}\n",
    "\n",
    "for i in df_train.index:\n",
    "    row = df_train.loc[i]\n",
    "    imgs_path = os.path.join(path, row['path'])\n",
    "    images = sorted([f for f in os.listdir(imgs_path) if \"._\" not in f])\n",
    "    for img in images:\n",
    "        #print(img)\n",
    "        if img[:-4] == 'incorrect_mask':\n",
    "            mask = 2 # incorrect\n",
    "        elif img[:-4] == 'normal':\n",
    "            mask = 1 # not wear\n",
    "        else:\n",
    "            mask = 0 # wear\n",
    "\n",
    "        classnum, age = return_class_simple(row, mask)\n",
    "        \n",
    "\n",
    "        df_train_class.loc[idx] = [row['id'], row['gender'], age_dict[age], mask_dict[mask], classnum, os.path.join(imgs_path, img)]\n",
    "        idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70a375fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_class.to_csv(\"./train_with_class.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e3e85",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a731ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_class = pd.read_csv(\"./train_with_class.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab72becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_train_class[:11340]\n",
    "valid_df = df_train_class[11341:15120]\n",
    "test_df = df_train_class[15121:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd9cb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>per_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>mask</th>\n",
       "      <th>class</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>30 to 60</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>16</td>\n",
       "      <td>../../input/data/train/images/000001_female_As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>30 to 60</td>\n",
       "      <td>wear</td>\n",
       "      <td>4</td>\n",
       "      <td>../../input/data/train/images/000001_female_As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>30 to 60</td>\n",
       "      <td>wear</td>\n",
       "      <td>4</td>\n",
       "      <td>../../input/data/train/images/000001_female_As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>30 to 60</td>\n",
       "      <td>wear</td>\n",
       "      <td>4</td>\n",
       "      <td>../../input/data/train/images/000001_female_As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>30 to 60</td>\n",
       "      <td>wear</td>\n",
       "      <td>4</td>\n",
       "      <td>../../input/data/train/images/000001_female_As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11335</th>\n",
       "      <td>11335</td>\n",
       "      <td>003785</td>\n",
       "      <td>female</td>\n",
       "      <td>30 to 60</td>\n",
       "      <td>wear</td>\n",
       "      <td>4</td>\n",
       "      <td>../../input/data/train/images/003785_female_As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11336</th>\n",
       "      <td>11336</td>\n",
       "      <td>003785</td>\n",
       "      <td>female</td>\n",
       "      <td>30 to 60</td>\n",
       "      <td>wear</td>\n",
       "      <td>4</td>\n",
       "      <td>../../input/data/train/images/003785_female_As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11337</th>\n",
       "      <td>11337</td>\n",
       "      <td>003785</td>\n",
       "      <td>female</td>\n",
       "      <td>30 to 60</td>\n",
       "      <td>wear</td>\n",
       "      <td>4</td>\n",
       "      <td>../../input/data/train/images/003785_female_As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11338</th>\n",
       "      <td>11338</td>\n",
       "      <td>003785</td>\n",
       "      <td>female</td>\n",
       "      <td>30 to 60</td>\n",
       "      <td>wear</td>\n",
       "      <td>4</td>\n",
       "      <td>../../input/data/train/images/003785_female_As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11339</th>\n",
       "      <td>11339</td>\n",
       "      <td>003785</td>\n",
       "      <td>female</td>\n",
       "      <td>30 to 60</td>\n",
       "      <td>not wear</td>\n",
       "      <td>10</td>\n",
       "      <td>../../input/data/train/images/003785_female_As...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11340 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  per_id  gender       age       mask  class  \\\n",
       "0          0  000001  female  30 to 60  incorrect     16   \n",
       "1          1  000001  female  30 to 60       wear      4   \n",
       "2          2  000001  female  30 to 60       wear      4   \n",
       "3          3  000001  female  30 to 60       wear      4   \n",
       "4          4  000001  female  30 to 60       wear      4   \n",
       "...      ...     ...     ...       ...        ...    ...   \n",
       "11335  11335  003785  female  30 to 60       wear      4   \n",
       "11336  11336  003785  female  30 to 60       wear      4   \n",
       "11337  11337  003785  female  30 to 60       wear      4   \n",
       "11338  11338  003785  female  30 to 60       wear      4   \n",
       "11339  11339  003785  female  30 to 60   not wear     10   \n",
       "\n",
       "                                                    path  \n",
       "0      ../../input/data/train/images/000001_female_As...  \n",
       "1      ../../input/data/train/images/000001_female_As...  \n",
       "2      ../../input/data/train/images/000001_female_As...  \n",
       "3      ../../input/data/train/images/000001_female_As...  \n",
       "4      ../../input/data/train/images/000001_female_As...  \n",
       "...                                                  ...  \n",
       "11335  ../../input/data/train/images/003785_female_As...  \n",
       "11336  ../../input/data/train/images/003785_female_As...  \n",
       "11337  ../../input/data/train/images/003785_female_As...  \n",
       "11338  ../../input/data/train/images/003785_female_As...  \n",
       "11339  ../../input/data/train/images/003785_female_As...  \n",
       "\n",
       "[11340 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11d7e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출처: https://github.com/utkuozbulak/pytorch-custom-dataset-examples/blob/master/src/custom_dataset_from_file.py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset  # For custom datasets\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "\n",
    "\n",
    "transform = transforms.Compose([Resize((512, 384), Image.BILINEAR),\n",
    "                                ToTensor(),\n",
    "                                Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2))])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df_train, transform, train=True):\n",
    "        # Get image list\n",
    "        self.image_list = df_train['path'].tolist()\n",
    "        self.target = df_train['class'].tolist()\n",
    "        # Calculate len\n",
    "        self.data_len = len(self.image_list)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get image name from the pandas df\n",
    "        single_image_path = self.image_list[index]\n",
    "        # Open image\n",
    "        # Open image\n",
    "        image = Image.open(single_image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(image)\n",
    "    \n",
    "        if self.train:\n",
    "            label = self.target[index]\n",
    "            \n",
    "            return (img, torch.tensor(label))\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb2be9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9db19643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=18, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "\n",
    "resnet18_pretrained = models.resnet18(pretrained=True)\n",
    "\n",
    "num_classes = 18\n",
    "num_ftrs = resnet18_pretrained.fc.in_features\n",
    "resnet18_pretrained.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "print(resnet18_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7be9bcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "   \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataset import Dataset  # For custom datasets\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Dataset variant 3:\n",
    "    # Read images from a folder, image classes are embedded in file names\n",
    "    # No csv is used whatsoever\n",
    "    # No torch transformations are used\n",
    "    # Preprocessing operations are defined inside the dataset\n",
    "    custom_mnist_from_file = CustomDataset(train_df, transform = transform)\n",
    "\n",
    "    mn_dataset_loader = torch.utils.data.DataLoader(dataset=custom_mnist_from_file,\n",
    "                                                    batch_size=2,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "\n",
    "    model = resnet18_pretrained\n",
    "    model = model.to(device)\n",
    "    #model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    running_loss = 0\n",
    "    for epoch in range(2): \n",
    "        for i, (images, labels) in enumerate(mn_dataset_loader):\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "            \n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if i % 2000 == 1999:\n",
    "                print('Epoch: %d Batch ID:%d Loss:%f' %(epoch, i, running_loss/2000))\n",
    "            \n",
    "\n",
    "    print('A single forward-backward pass is done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1fde02",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca432d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4c879a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = CustomDatasetFromFile(valid_df)\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                            batch_size=10,\n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8ea463a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "all_predictions = []\n",
    "for images, labels in valid_dataloader:\n",
    "    with torch.no_grad():\n",
    "        #images = images.to(device)\n",
    "        pred = model(images.float())\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        targets.extend(labels.numpy())\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5a846683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5992945326278659\n",
      "f1 0.33533191609193297\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "print('accuracy', metrics.accuracy_score(targets, all_predictions) )\n",
    "print('f1', np.mean(metrics.f1_score(targets, all_predictions, average=None)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c0918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
